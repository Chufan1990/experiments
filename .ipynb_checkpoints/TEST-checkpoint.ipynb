{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2d593d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import json\n",
    "import gpytorch\n",
    "import torch\n",
    "import tqdm\n",
    "import os\n",
    "import threading\n",
    "import concurrent\n",
    "import logging\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from matplotlib import pyplot as plt\n",
    "from time import sleep\n",
    "from typing import List, Union\n",
    "from math import ceil\n",
    "from data import GaussianPreprocessor, SequenceDataset, SequencePredictionDataset, SequenceReconstructionDataset\n",
    "from models import LSTMFeatureExtractor, GaussianProcessLayer, GPModel\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2aea4471",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = os.getenv(\"USERNAME\")\n",
    "\n",
    "task = 1876652\n",
    "\n",
    "data_paths= [f\"/home/{username}/workspace/dataset_downloader/{task}/processed\"]\n",
    "\n",
    "include_dirs=[\n",
    "5798514803372,  5798515131052,  5798515442348,  5798515778220,  5798516081324,  5798516392620,\n",
    "5798514811564,  5798515139244,  5798515450540,  5798515786412,  5798516089516,  5798516400812,\n",
    "5798514827948,  5798515147436,  5798515458732,  5798515794604,  5798516097708,  5798516409004,\n",
    "5798514836140,  5798515155628,  5798515466924,  5798515802796,  5798516105900,  5798516417196,\n",
    "5798514844332,  5798515172012,  5798515475116,  5798515810988,  5798516114092,  5798516425388,\n",
    "5798514852524,  5798515180204,  5798515483308,  5798515819180,  5798516122284,  5798516433580,\n",
    "5798514860716,  5798515188396,  5798515491500,  5798515827372,  5798516130476,  5798516441772,\n",
    "5798514877100,  5798515196588,  5798515499692,  5798515835564,  5798516138668,  5798516449964,\n",
    "]\n",
    "\n",
    "backup_dirs =[\n",
    "5798514885292,  5798515204780,  5798515507884,  5798515843756,  5798516146860,  5798516458156,\n",
    "5798514893484,  5798515212972,  5798515524268,  5798515851948,  5798516155052,  5798516466348,\n",
    "5798514909868,  5798515221164,  5798515532460,  5798515860140,  5798516163244,  5798516474540,\n",
    "5798514918060,  5798515229356,  5798515540652,  5798515868332,  5798516171436,  5798516482732,\n",
    "5798514926252,  5798515237548,  5798515548844,  5798515876524,  5798516179628,  5798694355628,\n",
    "5798514934444,  5798515245740,  5798515557036,  5798515884716,  5798516187820,  5798694363820,\n",
    "5798514950828,  5798515253932,  5798515565228,  5798515892908,  5798516196012,  5798694478508,\n",
    "5798514959020,  5798515262124,  5798515581612,  5798515901100,  5798516204204,  5798694486700,\n",
    "5798514967212,  5798515270316,  5798515589804,  5798515909292,  5798516212396,  5798694494892,\n",
    "5798514975404,  5798515278508,  5798515597996,  5798515917484,  5798516220588,  5798694503084,\n",
    "5798514983596,  5798515286700,  5798515606188,  5798515925676,  5798516228780,  5798694511276,\n",
    "5798514999980,  5798515294892,  5798515614380,  5798515933868,  5798516236972,  5798694519468,\n",
    "5798515008172,  5798515303084,  5798515638956,  5798515942060,  5798516245164,  5798694527660,\n",
    "5798515016364,  5798515311276,  5798515655340,  5798515950252,  5798516261548,  5798694535852,\n",
    "5798515024556,  5798515319468,  5798515663532,  5798515958444,  5798516269740,  5798694544044,\n",
    "5798515032748,  5798515327660,  5798515671724,  5798515966636,  5798516277932,  5798694552236,\n",
    "5798515040940,  5798515335852,  5798515688108,  5798515974828,  5798516286124,  5798694560428,\n",
    "5798515049132,  5798515344044,  5798515696300,  5798515983020,  5798516302508,  5798694568620,\n",
    "5798515057324,  5798515352236,  5798515704492,  5798515991212,  5798516310700,  5798694576812,\n",
    "5798515065516,  5798515360428,  5798515712684,  5798515999404,  5798516318892,  5798694585004,\n",
    "5798515073708,  5798515368620,  5798515720876,  5798516007596,  5798516327084,  5798694617772,\n",
    "5798515081900,  5798515385004,  5798515729068,  5798516015788,  5798516335276,  5798694625964,\n",
    "5798515090092,  5798515393196,  5798515737260,  5798516023980,  5798516343468,  5798694634156,\n",
    "5798515098284,  5798515401388,  5798515745452,  5798516032172,  5798516351660,  5798694642348,\n",
    "5798515106476,  5798515409580,  5798515753644,  5798516048556,  5798516368044,\n",
    "5798515114668,  5798515417772,  5798515761836,  5798516056748,  5798516376236,\n",
    "5798515122860,  5798515434156,  5798515770028,  5798516064940,  5798516384428,  \n",
    "]\n",
    "\n",
    "include_dirs = set(include_dirs)\n",
    "\n",
    "include_dirs = [str(x) for x in include_dirs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "beb3d9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = ['engineRpm', 'speedMps', 'driveMotorTorqueNm', 'throttle', 'brake', \"linearVelocityVrf_y\", \"linearAccelerationVrf_y\"]\n",
    "# normalize_features = ['engineRpm', 'speedMps', 'driveMotorTorqueNm', 'throttle', 'brake', \"linearVelocityVrf_y\", \"linearAccelerationVrf_y\"]\n",
    "# onehot_features = None\n",
    "# lb = np.array([0.000000, 0.000000, -70.000000, 0.0, 0.0, -0.3, -10, -0.3, -10])\n",
    "# ub = np.array([4300.0, 24.0, 255.000000, 100.0, 100.0, 22.5, 10, 22.5, 10])\n",
    "\n",
    "features = {\"env_learning_zip/chassis.txt\": ['engineRpm', 'speedMps', 'driveMotorTorqueNm'],\n",
    "            \"env_learning_zip/localization.txt\":  [\"linearVelocityVrf_y\", \"linearAccelerationVrf_y\"],\n",
    "            \"env_learning_zip/control.txt\":  [\"throttle\", \"brake\"]\n",
    "           }\n",
    "\n",
    "normalize_features = ['engineRpm', 'speedMps', 'driveMotorTorqueNm', \"linearVelocityVrf_y\", \"linearAccelerationVrf_y\", \"throttle\"]\n",
    "onehot_features = None\n",
    "lb = np.array([0.000000, 0.000000, -70.000000, -0.3, -10.0, -100.0])\n",
    "ub = np.array([4300.0, 24.0, 255.000000, 22.5, 5.0, 100.0, ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f0851cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, in_memory=False, workers=16, features=None):\n",
    "        self.in_memory = in_memory\n",
    "        self.workers = workers\n",
    "        self.features = features\n",
    "\n",
    "args = Args(in_memory=False,  workers=16, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5d70504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "10e58e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 5\n",
    "hidden_size = 64\n",
    "batch_size = 100\n",
    "seq_length = 10\n",
    "bidirectional = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ec8fdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 48/48 [00:56<00:00,  1.17s/it]\n"
     ]
    }
   ],
   "source": [
    "preprocessor = GaussianPreprocessor(data_paths, \n",
    "                                    features=features, \n",
    "                                    normalize_features=normalize_features,\n",
    "                                    lb=lb,\n",
    "                                    ub=ub\n",
    "                                   )\n",
    "\n",
    "dataset = SequenceReconstructionDataset(data_paths, \n",
    "                                        batch_size=batch_size,\n",
    "                                        seq_length=seq_length,\n",
    "                                        preprocessor=preprocessor, \n",
    "                                        include_dirs=include_dirs,\n",
    "                                        args=args,\n",
    "                                        files=[\"env_learning_zip/chassis.txt\", \n",
    "                                               \"env_learning_zip/localization.txt\", \n",
    "                                               \"env_learning_zip/control.txt\"],\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "55294b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = LSTMFeatureExtractor(input_size, hidden_size, bidirectional=bidirectional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6d72400a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1629,  0.8797, -0.3723,  0.9141,  0.3956,  0.1549],\n",
      "         [ 0.1628,  0.8797, -0.3723,  0.9137,  0.4105,  0.1174],\n",
      "         [ 0.1627,  0.8780, -0.3538,  0.9132,  0.4286,  0.1496],\n",
      "         [ 0.1624,  0.8780, -0.3538,  0.9126,  0.4399,  0.1406],\n",
      "         [ 0.1622,  0.8780, -0.3538,  0.9126,  0.4234,  0.1349],\n",
      "         [ 0.1622,  0.8780, -0.3538,  0.9128,  0.4032,  0.1571],\n",
      "         [ 0.1622,  0.8780, -0.3538,  0.9132,  0.3657,  0.1382],\n",
      "         [ 0.1622,  0.8780, -0.3354,  0.9138,  0.3370,  0.1498],\n",
      "         [ 0.1626,  0.8780, -0.3354,  0.9136,  0.3281,  0.1738],\n",
      "         [ 0.1630,  0.8780, -0.3169,  0.9129,  0.3332,  0.1089]]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d6359b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DKLModel(gpytorch.Module):\n",
    "    def __init__(self, feature_extractor, feature_size, grid_bounds=(-1, 1), device=torch.device('cpu')):\n",
    "        super(DKLModel, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.inducing_points = torch.randn(1024, feature_size, dtype=torch.float).to(device)\n",
    "        self.gp_layer = GPModel(inducing_points=self.inducing_points)\n",
    "        self.grid_bounds = grid_bounds\n",
    "        \n",
    "        self.scale_to_bounds = gpytorch.utils.grid.ScaleToBounds(self.grid_bounds[0], self.grid_bounds[1])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)[:, -1, :]\n",
    "        features = self.scale_to_bounds(features)\n",
    "        res = self.gp_layer(features)\n",
    "        return res\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3adccf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DKLModel(feature_extractor, feature_size=hidden_size, device=device)\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "\n",
    "# If you run this example without CUDA, I hope you like waiting!\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    likelihood = likelihood.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2346321e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0456dda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  12%|████▍                               | 37/300 [8:07:22<50:56:23, 697.28s/it, loss=-.0572]"
     ]
    }
   ],
   "source": [
    "smoke_test = ('CI' in os.environ)\n",
    "num_epochs = 1 if smoke_test else 300\n",
    "num_batches = 100\n",
    "\n",
    "\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr=0.1)\n",
    "\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "\n",
    "# Our loss object. We're using the VariationalELBO, which essentially just computes the ELBO\n",
    "# mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model.gp_layer, num_data=batch_size)\n",
    "# mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=1000)\n",
    "\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "\n",
    "# We use more CG iterations here because the preconditioner introduced in the NeurIPS paper seems to be less\n",
    "# effective for VI.\n",
    "epochs_iter = tqdm.tqdm(range(num_epochs), desc=\"Epoch\")\n",
    "for i in epochs_iter:\n",
    "    # Within each iteration, we will go over each minibatch of data\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    start_index = 0\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    batch_index = 0\n",
    "    for batch in dataset.batches():\n",
    "        if batch_index == num_batches:\n",
    "            break\n",
    "        batch_index += 1\n",
    "        \n",
    "        target = batch[:, -2, -1].to(device, dtype=torch.float)\n",
    "        state = batch[:, :, :-1].to(device, dtype=torch.float)\n",
    "        output = model(state)\n",
    "        loss = -mll(output, target)\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "        writer.add_scalar('train MAE', torch.mean(torch.abs(output.mean - target)))\n",
    "        \n",
    "    total_loss = sum(losses) / num_batches\n",
    "    epochs_iter.set_postfix(loss=total_loss)\n",
    "    writer.add_scalar('Loss/train', total_loss, i)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8fb0e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
