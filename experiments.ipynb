{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d593d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import json\n",
    "import gpytorch\n",
    "import torch\n",
    "import tqdm\n",
    "import os\n",
    "import threading\n",
    "import concurrent\n",
    "import logging\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from matplotlib import pyplot as plt\n",
    "from time import sleep\n",
    "from typing import List, Union\n",
    "from math import ceil\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1063fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cc96604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[raw data shape] (14980, 15)\n",
      "[data shape after thinning] torch.Size([749, 14])\n"
     ]
    }
   ],
   "source": [
    "def flatten_dict(x: dict, prefix=\"\"):\n",
    "    y = {}\n",
    "    for k, v in x.items():\n",
    "        if isinstance(v, dict):\n",
    "            y.update(flatten_dict(v, k + \"_\"))\n",
    "        else:\n",
    "            y[prefix + k] = v\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58923a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "class ReturnValueThread(threading.Thread):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.result = None\n",
    "\n",
    "    def run(self):\n",
    "        if self._target is None:\n",
    "            return  # could alternatively raise an exception, depends on the use case\n",
    "        try:\n",
    "            self.result = self._target(*self._args, **self._kwargs)\n",
    "        except Exception as exc:\n",
    "            print(f'{type(exc).__name__}: {exc}', file=sys.stderr)  # properly handle the exception\n",
    "\n",
    "    def join(self, *args, **kwargs):\n",
    "        super().join(*args, **kwargs)\n",
    "        return self.result\n",
    "    \n",
    "def preprocess(data_paths, include_dirs, max_workers: int = 64):\n",
    "        \n",
    "    all_tasks = []\n",
    "    num_tasks = len(include_dirs)\n",
    "        \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for path in data_paths:\n",
    "            for folder in os.listdir(path):\n",
    "                all_tasks.append(executor.submit(subprocess, path, folder))\n",
    "            for future in tqdm.tqdm(concurrent.futures.as_completed(all_tasks), total=num_tasks):\n",
    "                try:\n",
    "                    data = future.result()\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logging.error(e)\n",
    "\n",
    "def subprocess(path, folder):   \n",
    "\n",
    "    if int(folder) not in include_dirs:\n",
    "        return\n",
    "\n",
    "    chassis_file = os.path.join(path, folder, \"env_learning_zip/chassis.txt\")\n",
    "    control_file = os.path.join(path, folder, \"env_learning_zip/control.txt\")\n",
    "    localization_file = os.path.join(path, folder, \"env_learning_zip/localization.txt\")\n",
    "    \n",
    "    a= num_lines(chassis_file)\n",
    "    b= num_lines(control_file)\n",
    "    c= num_lines(localization_file)\n",
    "\n",
    "    if a != b or b != c or c != a or a == 0:\n",
    "        return\n",
    "\n",
    "    to_csv(chassis_file, os.path.join(path, folder, \"preprocessed/chassis.csv\"))\n",
    "    to_csv(control_file, os.path.join(path, folder, \"preprocessed/control.csv\"))\n",
    "    to_csv(localization_file, os.path.join(path, folder, \"preprocessed/localization.csv\"))\n",
    "\n",
    "\n",
    "    \n",
    "def to_csv(file, dest):\n",
    "    df = pd.DataFrame()\n",
    "    with open(file) as file:\n",
    "        for line in file:\n",
    "            nested_dict = json.loads(line)\n",
    "            df = pd.concat([df, pd.DataFrame.from_dict(flatten_dict(nested_dict), orient='columns')], axis=0)\n",
    "    df.to_csv(dest)\n",
    "\n",
    "def num_lines(abs_path):\n",
    "    num = 0\n",
    "    if not os.path.exists(abs_path):\n",
    "        return num\n",
    "\n",
    "    with open(abs_path) as file:\n",
    "        for line in file:\n",
    "            num += 1\n",
    "\n",
    "    return num\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbac01f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_paths,\n",
    "        include_dirs=None,\n",
    "        batch_size=1000\n",
    "    ):\n",
    "        self.data_paths = data_paths\n",
    "        self.chassis_df = pd.DataFrame()\n",
    "        self.control_df = pd.DataFrame()\n",
    "        self.localization_df = pd.DataFrame()\n",
    "        self.include_dirs = include_dirs\n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "    def load_data(self):\n",
    "        batch_index = 0\n",
    "        tmp1 = []\n",
    "        tmp2 = []\n",
    "        tmp3 = []\n",
    "        tmp4 = []\n",
    "        \n",
    "        with tqdm.notebook.tqdm(total=len(self.include_dirs)) as pbar:\n",
    "            pbar.set_description('Processing:')\n",
    "            for p in self.data_paths:\n",
    "                for folder in os.listdir(p):\n",
    "                    pbar.update(1)\n",
    "                    if folder not in self.include_dirs:\n",
    "                        continue\n",
    "\n",
    "                    chassis_file = os.path.join(p, folder, \"env_learning_zip/chassis.txt\")\n",
    "                    control_file = os.path.join(p, folder, \"env_learning_zip/control.txt\")\n",
    "                    localization_file = os.path.join(p, folder, \"env_learning_zip/localization.txt\")\n",
    "\n",
    "                    if not os.path.exists(chassis_file) or \\\n",
    "                    not os.path.exists(control_file) or \\\n",
    "                    not os.path.exists(localization_file):\n",
    "                        continue\n",
    "\n",
    "\n",
    "                    a = num_lines(chassis_file)\n",
    "                    b = num_lines(control_file)\n",
    "                    c = num_lines(localization_file)\n",
    "\n",
    "                    if a != b or b != c or a == 0:\n",
    "                        continue\n",
    "\n",
    "                    with open(chassis_file) as file:\n",
    "                        for line in file:\n",
    "                            tmp1.append(flatten_dict(json.loads(line)))\n",
    "\n",
    "                    with open(control_file) as file:\n",
    "                        for line in file:\n",
    "                            tmp2.append(flatten_dict(json.loads(line)))\n",
    "\n",
    "                    with open(localization_file) as file:\n",
    "                        for line in file:\n",
    "                            tmp3.append(flatten_dict(json.loads(line)))\n",
    "\n",
    "                    if a < self.batch_size:\n",
    "\n",
    "                        tmp4.append(np.ones(a) * batch_index)\n",
    "                        batch_index += 1\n",
    "                        continue\n",
    "\n",
    "                    num_batches = ceil(a / self.batch_size)\n",
    "\n",
    "                    for i in range(num_batches - 1):\n",
    "                        tmp4.append(np.ones(self.batch_size) * batch_index)\n",
    "                        batch_index += 1\n",
    "\n",
    "                    tmp4.append(np.ones(a - (num_batches - 1) * self.batch_size) * batch_index)\n",
    "                    batch_index += 1\n",
    "\n",
    "        tmp4 = np.concatenate(tmp4, axis=0)\n",
    "    \n",
    "        self.chassis_df = pd.concat([pd.DataFrame(tmp1), pd.DataFrame(tmp4, columns=[\"batch\"])], axis=1)\n",
    "        self.control_df = pd.concat([pd.DataFrame(tmp2), pd.DataFrame(tmp4, columns=[\"batch\"])], axis=1)        \n",
    "        self.localization_df = pd.concat([pd.DataFrame(tmp3), pd.DataFrame(tmp4, columns=[\"batch\"])], axis=1)\n",
    "        return self.chassis_df.reset_index(drop=True), self.control_df.reset_index(drop=True), self.localization_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59a87d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveUnwantedData(x: pd.DataFrame):\n",
    "    index = x[\"drivingMode\"] == \"COMPLETE_AUTO_DRIVE\"\n",
    "    x = x[index]\n",
    "    return x, index\n",
    "\n",
    "def AddRelativeTime(x: pd.DataFrame):\n",
    "    if \"dt\" in x.columns:\n",
    "        return x\n",
    "    \n",
    "    if \"timestamp\" not in x.columns:\n",
    "        raise Exception(\"Not timestamp\")\n",
    "        \n",
    "    num_data_point = x.shape[0]\n",
    "        \n",
    "    relative_time = x.iloc[1:num_data_point][\"timestamp\"].to_numpy() - x.iloc[0:num_data_point-1][\"timestamp\"].to_numpy()\n",
    "    relative_time = np.append(relative_time, 0.0)   \n",
    "    \n",
    "    x['dt'] = relative_time\n",
    "    return x\n",
    "\n",
    "def TimeSeriesData(raw_data: pd.DataFrame, state_columns: List[str], observ_columns: List[str]):\n",
    "    \n",
    "    data = None\n",
    "    if \"dt\" not in raw_data.columns:\n",
    "        data = AddRelativeTime(raw_data)\n",
    "    else:\n",
    "        data = raw_data\n",
    "        \n",
    "    num_data_point = data.shape[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    if(any(entity not in data.columns for entity in state_columns)):\n",
    "        raise Exception(\"Unknown state\")\n",
    "    \n",
    "    if(any(entity not in data.columns for entity in observ_columns)):\n",
    "        raise Exception(\"Unknown observation\")\n",
    "    \n",
    "    x_index = np.logical_and([data[\"dt\"] < 0.5], [data[\"dt\"] > 0.0]).flatten()\n",
    "    y_index = np.append(x_index, x_index[-1])[1:].flatten()\n",
    "        \n",
    "    X = data[x_index][state_columns + [\"dt\"]]\n",
    "    Y = data[y_index][observ_columns]\n",
    "\n",
    "        \n",
    "    return X, Y, x_index, y_index\n",
    "\n",
    "def NormalizeData(x: pd.DataFrame, shifter: Union[None, np.array] = None, normalizer: Union[None, np.array] = None, has_batch=True):\n",
    "    X = x\n",
    "    if has_batch:\n",
    "        X = x[x.columns[x.columns != \"batch\"]]\n",
    "        batch = x[\"batch\"]\n",
    "    \n",
    "    if shifter is None:\n",
    "        shifter = X.min()\n",
    "        \n",
    "    X = X - shifter\n",
    "    \n",
    "    if normalizer is None:\n",
    "        normalizer = X.max()\n",
    "\n",
    "    X = 2 * (X / normalizer) - 1\n",
    "    \n",
    "    if has_batch:\n",
    "        return pd.concat([X.clip(-1, 1), batch], axis=1), shifter, normalizer\n",
    "    else:\n",
    "        return X.clip(-1, 1), shifter, normalizer\n",
    "\n",
    "def DeNormalizeData(x: np.array, shifter: np.array, normalizer: np.array):\n",
    "    return (x + 1.0) / 2.0 * normalizer + shifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aea4471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMEncoderDecoder(\n",
      "  (lstm_encoder): LSTMFeatureExtractor(\n",
      "    (rnn): LSTM(14, 64, num_layers=2, dropout=0.2, bidirectional=True)\n",
      "  )\n",
      "  (lstm_decoder): LSTMFeatureExtractor(\n",
      "    (rnn): LSTM(128, 64, proj_size=14, num_layers=2, dropout=0.2)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "username = os.getenv(\"USERNAME\")\n",
    "\n",
    "task = 1876652\n",
    "\n",
    "data_paths= [f\"/home/{username}/workspace/dataset_downloader/{task}/processed\"]\n",
    "\n",
    "include_dirs=[\n",
    "5798514803372,  5798515131052,  5798515442348,  5798515778220,  5798516081324,  5798516392620,\n",
    "5798514811564,  5798515139244,  5798515450540,  5798515786412,  5798516089516,  5798516400812,\n",
    "5798514827948,  5798515147436,  5798515458732,  5798515794604,  5798516097708,  5798516409004,\n",
    "5798514836140,  5798515155628,  5798515466924,  5798515802796,  5798516105900,  5798516417196,\n",
    "5798514844332,  5798515172012,  5798515475116,  5798515810988,  5798516114092,  5798516425388,\n",
    "5798514852524,  5798515180204,  5798515483308,  5798515819180,  5798516122284,  5798516433580,\n",
    "5798514860716,  5798515188396,  5798515491500,  5798515827372,  5798516130476,  5798516441772,\n",
    "5798514877100,  5798515196588,  5798515499692,  5798515835564,  5798516138668,  5798516449964,\n",
    "5798514885292,  5798515204780,  5798515507884,  5798515843756,  5798516146860,  5798516458156,\n",
    "5798514893484,  5798515212972,  5798515524268,  5798515851948,  5798516155052,  5798516466348,\n",
    "5798514909868,  5798515221164,  5798515532460,  5798515860140,  5798516163244,  5798516474540,\n",
    "5798514918060,  5798515229356,  5798515540652,  5798515868332,  5798516171436,  5798516482732,\n",
    "5798514926252,  5798515237548,  5798515548844,  5798515876524,  5798516179628,  5798694355628,\n",
    "]\n",
    "\n",
    "backup_dirs =[\n",
    "5798514934444,  5798515245740,  5798515557036,  5798515884716,  5798516187820,  5798694363820,\n",
    "5798514950828,  5798515253932,  5798515565228,  5798515892908,  5798516196012,  5798694478508,\n",
    "5798514959020,  5798515262124,  5798515581612,  5798515901100,  5798516204204,  5798694486700,\n",
    "5798514967212,  5798515270316,  5798515589804,  5798515909292,  5798516212396,  5798694494892,\n",
    "5798514975404,  5798515278508,  5798515597996,  5798515917484,  5798516220588,  5798694503084,\n",
    "5798514983596,  5798515286700,  5798515606188,  5798515925676,  5798516228780,  5798694511276,\n",
    "5798514999980,  5798515294892,  5798515614380,  5798515933868,  5798516236972,  5798694519468,\n",
    "5798515008172,  5798515303084,  5798515638956,  5798515942060,  5798516245164,  5798694527660,\n",
    "5798515016364,  5798515311276,  5798515655340,  5798515950252,  5798516261548,  5798694535852,\n",
    "5798515024556,  5798515319468,  5798515663532,  5798515958444,  5798516269740,  5798694544044,\n",
    "5798515032748,  5798515327660,  5798515671724,  5798515966636,  5798516277932,  5798694552236,\n",
    "5798515040940,  5798515335852,  5798515688108,  5798515974828,  5798516286124,  5798694560428,\n",
    "5798515049132,  5798515344044,  5798515696300,  5798515983020,  5798516302508,  5798694568620,\n",
    "5798515057324,  5798515352236,  5798515704492,  5798515991212,  5798516310700,  5798694576812,\n",
    "5798515065516,  5798515360428,  5798515712684,  5798515999404,  5798516318892,  5798694585004,\n",
    "5798515073708,  5798515368620,  5798515720876,  5798516007596,  5798516327084,  5798694617772,\n",
    "5798515081900,  5798515385004,  5798515729068,  5798516015788,  5798516335276,  5798694625964,\n",
    "5798515090092,  5798515393196,  5798515737260,  5798516023980,  5798516343468,  5798694634156,\n",
    "5798515098284,  5798515401388,  5798515745452,  5798516032172,  5798516351660,  5798694642348,\n",
    "5798515106476,  5798515409580,  5798515753644,  5798516048556,  5798516368044,\n",
    "5798515114668,  5798515417772,  5798515761836,  5798516056748,  5798516376236,\n",
    "5798515122860,  5798515434156,  5798515770028,  5798516064940,  5798516384428    \n",
    "]\n",
    "\n",
    "include_dirs = set(include_dirs)\n",
    "\n",
    "include_dirs = [str(x) for x in include_dirs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ec8fdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(data_paths, include_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83de41a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0b6b387f894aea84adf0c0e8c1cf9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if os.path.exists(f'/home/{username}/workspace/dataset_downloader/{task}/processed/preprocessed/chassis_training.csv') and \\\n",
    "os.path.exists(f'/home/{username}/workspace/dataset_downloader/{task}/processed/preprocessed/control_training.csv') and \\\n",
    "os.path.exists(f'/home/{username}/workspace/dataset_downloader/{task}/processed/preprocessed/localization_training.csv'):\n",
    "    chassis_df = pd.read_csv(f'/home/{username}/workspace/dataset_downloader/{task}/processed/preprocessed/chassis_training.csv')\n",
    "    control_df = pd.read_csv(f'/home/{username}/workspace/dataset_downloader/{task}/processed/preprocessed/control_training.csv')\n",
    "    localization_df = pd.read_csv(f'/home/{username}/workspace/dataset_downloader/{task}/processed/preprocessed/localization_training.csv')\n",
    "\n",
    "else:\n",
    "    chassis_df, control_df, localization_df = dl.load_data()\n",
    "\n",
    "    os.makedirs(f'/home/{username}/workspace/dataset_downloader/{task}/processed/preprocessed/', exist_ok=True)  \n",
    "    chassis_df.to_csv(f'/home/{username}/workspace/dataset_downloader/{task}/processed/preprocessed/chassis_training.csv')\n",
    "    control_df.to_csv(f'/home/{username}/workspace/dataset_downloader/{task}/processed/preprocessed/control_training.csv')\n",
    "    localization_df.to_csv(f'/home/{username}/workspace/dataset_downloader/{task}/processed/preprocessed/localization_training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0282c331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2235, -0.0534, -0.0919, -0.2298, -0.0076, -0.1813, -0.5084, -0.0755,\n",
      "         -0.1404, -0.2393, -0.0631, -0.3437, -0.1596, -0.1141],\n",
      "        [-0.2046, -0.0358, -0.0195, -0.2164,  0.1551, -0.1185, -0.5436, -0.0204,\n",
      "         -0.1387, -0.2142, -0.0543, -0.3537, -0.1565, -0.0977],\n",
      "        [-0.1383,  0.0428,  0.0341, -0.0823,  0.2592,  0.0322, -0.4231,  0.0606,\n",
      "         -0.0928, -0.1328, -0.0286, -0.3320, -0.0992, -0.0806],\n",
      "        [-0.2175, -0.0401, -0.0439, -0.2083,  0.0901, -0.0782, -0.5182,  0.0093,\n",
      "         -0.0894, -0.1775, -0.0429, -0.3433, -0.1478, -0.0808],\n",
      "        [-0.2256, -0.0243, -0.0279, -0.2059,  0.1067, -0.1033, -0.5236,  0.0020,\n",
      "         -0.0871, -0.2036, -0.0162, -0.3664, -0.1454, -0.1068]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "tensor([[ 4.2850e-01,  4.5847e-01,  3.2603e-01,  3.8882e-01, -4.7573e-01,\n",
      "         -6.6099e-01, -8.7429e-02, -1.0739e+00, -3.5041e-01,  1.5334e-01,\n",
      "         -1.6457e+00,  3.3230e-01,  2.2536e-01,  2.9109e-01],\n",
      "        [-2.5319e-01, -1.5004e-01, -7.0540e-01, -2.0315e-03, -2.5928e-01,\n",
      "         -4.5153e-01, -4.3102e-01, -1.3705e+00, -9.7829e-01, -8.7506e-01,\n",
      "         -2.8092e+00, -9.5168e-01, -4.0947e-01, -5.6105e-01],\n",
      "        [ 2.0454e+00,  2.4322e+00,  1.3575e+00,  2.2218e+00, -4.7573e-01,\n",
      "         -1.7525e+00, -9.4668e-01, -2.3349e+00, -2.6668e+00, -1.7750e+00,\n",
      "         -1.7850e+00,  1.0351e+00,  7.1251e-01,  1.7891e+00],\n",
      "        [ 3.9161e+00,  2.4688e+00,  3.2536e+00,  2.8231e+00, -1.9917e+00,\n",
      "         -2.0462e+00, -7.4611e-01, -7.1819e-02,  1.9940e-01,  1.7602e+00,\n",
      "          8.4419e-01,  3.6948e+00,  3.3849e+00,  3.9069e+00],\n",
      "        [ 1.5375e+00, -5.3737e-01,  2.9293e-01,  4.1870e-01, -2.4783e+00,\n",
      "         -1.4162e+00,  5.6154e-02,  1.0409e+00,  1.2209e+00,  2.0816e+00,\n",
      "          3.7907e-01,  1.4941e+00,  3.0011e+00,  2.3315e+00]])\n"
     ]
    }
   ],
   "source": [
    "chassis_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0761a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chassis_features = ['engineRpm', \n",
    "                    'speedMps', \n",
    "                    'driveMotorTorqueNm']\n",
    "\n",
    "control_features = ['throttle', \n",
    "                    'brake']\n",
    "\n",
    "localization_features = [\"linearVelocityVrf_y\", \n",
    "                         \"linearAccelerationVrf_y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3d83d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chassis_df = chassis_df.reset_index(drop=True)\n",
    "control_df = control_df.reset_index(drop=True)\n",
    "localization_df = localization_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbf3720",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chassis_df.shape)\n",
    "print(control_df.shape)\n",
    "print(localization_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4dfc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "chassis_df_v, valid_index = RemoveUnwantedData(chassis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e683f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(valid_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49f398d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chassis_df_v = chassis_df_v.reset_index()\n",
    "control_df_v  = control_df[valid_index].reset_index()\n",
    "localization_df_v = localization_df[valid_index].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bcc4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_batch = pd.DataFrame()\n",
    "train_y_batch = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca90a3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = chassis_df[\"batch\"].unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9e6663",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in range(int(num_batches)):\n",
    "    chassis_b = chassis_df_v[chassis_df_v[\"batch\"] == b].reset_index(drop=True)\n",
    "    localization_b = localization_df_v[localization_df_v[\"batch\"] == b].reset_index(drop=True)\n",
    "    control_b = control_df_v[control_df_v[\"batch\"] == b].reset_index(drop=True)\n",
    "    \n",
    "    num_samples = chassis_b.shape[0] - 1\n",
    "    \n",
    "    train_x = pd.concat(\n",
    "        [chassis_b.loc[0:num_samples-1, chassis_features].reset_index(drop=True), \n",
    "         localization_b.loc[0:num_samples-1, localization_features].reset_index(drop=True), \n",
    "         localization_b.loc[1:num_samples, localization_features].reset_index(drop=True)], \n",
    "        ignore_index=True,\n",
    "        axis=1)\n",
    "    \n",
    "    train_x[\"batch\"] = b\n",
    "\n",
    "    train_y = pd.DataFrame(control_b.loc[0:num_samples-1, control_features[0]] - control_b.loc[0:num_samples-1, control_features[1]])\n",
    "    \n",
    "    train_y[\"batch\"] = b\n",
    "    print(train_y)\n",
    "    \n",
    "    \n",
    "    train_x_batch = pd.concat([train_x_batch, train_x], axis=0, ignore_index=True)\n",
    "    train_y_batch = pd.concat([train_y_batch, train_y], axis=0, ignore_index=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbc81b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514a5c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_shifter = np.array([0.000000, 0.000000, -70.000000, -0.3, -5, -0.3, -5])\n",
    "\n",
    "x_normalizer = np.array([4300.0, 24.0, 255.000000, 22.5, 10, 22.5, 10])\n",
    "\n",
    "y_shifter = np.array([-60])\n",
    "\n",
    "y_normalizer = np.array([120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e2746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_n, x_shifter, x_normalizer = NormalizeData(train_x_batch, x_shifter, x_normalizer)\n",
    "train_y_n, y_shifter, y_normalizer = NormalizeData(train_y_batch, y_shifter, y_normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5376d36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(train_y_n.shape) == 1:\n",
    "    train_y_n = train_y_n.reshape(train_y_n.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac1ec1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
