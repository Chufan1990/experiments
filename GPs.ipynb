{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "762a351e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msk\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import List, Union\n",
    "import os\n",
    "from matplotlib import colormaps\n",
    "import matplotlib as mpl\n",
    "import tqdm\n",
    "from typing import Dict, Any\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from data import GaussianPreprocessor, SequenceDataset, SequencePredictionDataset, SequenceReconstructionDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "import pyro\n",
    "from pyro.contrib.timeseries import IndependentMaternGP, LinearlyCoupledMaternGP, DependentMaternGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71953691",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFeatureExtractor(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: Dict[str, Union[int, List[int]]],\n",
    "        output_size: Dict[str, int],\n",
    "        dropout: Dict[str, float],\n",
    "        bidirectional: bool = True,\n",
    "        num_action: int = 0,\n",
    "    ):\n",
    "        super(CustomFeatureExtractor, self).__init__()\n",
    "        self.num_action = num_action\n",
    "\n",
    "        self.lstm_feature_extractor = LSTMFeatureExtractor(\n",
    "            input_size,\n",
    "            hidden_size[\"lstm\"],\n",
    "            output_size[\"lstm\"],\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout[\"lstm\"],\n",
    "        )\n",
    "        if num_action > 0:\n",
    "            self.linear_feature_extractor = MLPFeatureExtractor(\n",
    "                num_action,\n",
    "                hidden_size[\"linear\"],\n",
    "                output_size[\"linear\"],\n",
    "                dropout=dropout[\"linear\"],\n",
    "            )\n",
    "        self.output_layer = torch.nn.tanh(\n",
    "            torch.nn.Linear(\n",
    "                output_size[\"lstm\"] + output_size[\"linear\"], output_size[\"final\"]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, seq, a):\n",
    "        x = self.lstm_feature_extractor(seq)\n",
    "        if self.num_action > 0:\n",
    "            x = torch.cat(x, self.linear_feature_extractor(x), axis=1)\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4163966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = os.getenv(\"USERNAME\")\n",
    "\n",
    "task = 1876652\n",
    "\n",
    "data_paths= [f\"/home/{username}/workspace/dataset_downloader/{task}/processed\"]\n",
    "\n",
    "include_dirs=[\n",
    "5798514803372,  5798515131052,  5798515442348,  5798515778220,  5798516081324,  5798516392620,\n",
    "5798514811564,  5798515139244,  5798515450540,  5798515786412,  5798516089516,  5798516400812,\n",
    "5798514827948,  5798515147436,  5798515458732,  5798515794604,  5798516097708,  5798516409004,\n",
    "5798514836140,  5798515155628,  5798515466924,  5798515802796,  5798516105900,  5798516417196,\n",
    "5798514844332,  5798515172012,  5798515475116,  5798515810988,  5798516114092,  5798516425388,\n",
    "5798514852524,  5798515180204,  5798515483308,  5798515819180,  5798516122284,  5798516433580,\n",
    "5798514860716,  5798515188396,  5798515491500,  5798515827372,  5798516130476,  5798516441772,\n",
    "5798514877100,  5798515196588,  5798515499692,  5798515835564,  5798516138668,  5798516449964,\n",
    "5798514885292,  5798515204780,  5798515507884,  5798515843756,  5798516146860,  5798516458156,\n",
    "5798514893484,  5798515212972,  5798515524268,  5798515851948,  5798516155052,  5798516466348,\n",
    "5798514909868,  5798515221164,  5798515532460,  5798515860140,  5798516163244,  5798516474540,\n",
    "5798514918060,  5798515229356,  5798515540652,  5798515868332,  5798516171436,  5798516482732,\n",
    "5798514926252,  5798515237548,  5798515548844,  5798515876524,  5798516179628,  5798694355628,\n",
    "]\n",
    "\n",
    "backup_dirs =[\n",
    "5798514934444,  5798515245740,  5798515557036,  5798515884716,  5798516187820,  5798694363820,\n",
    "5798514950828,  5798515253932,  5798515565228,  5798515892908,  5798516196012,  5798694478508,\n",
    "5798514959020,  5798515262124,  5798515581612,  5798515901100,  5798516204204,  5798694486700,\n",
    "5798514967212,  5798515270316,  5798515589804,  5798515909292,  5798516212396,  5798694494892,\n",
    "5798514975404,  5798515278508,  5798515597996,  5798515917484,  5798516220588,  5798694503084,\n",
    "5798514983596,  5798515286700,  5798515606188,  5798515925676,  5798516228780,  5798694511276,\n",
    "5798514999980,  5798515294892,  5798515614380,  5798515933868,  5798516236972,  5798694519468,\n",
    "5798515008172,  5798515303084,  5798515638956,  5798515942060,  5798516245164,  5798694527660,\n",
    "5798515016364,  5798515311276,  5798515655340,  5798515950252,  5798516261548,  5798694535852,\n",
    "5798515024556,  5798515319468,  5798515663532,  5798515958444,  5798516269740,  5798694544044,\n",
    "5798515032748,  5798515327660,  5798515671724,  5798515966636,  5798516277932,  5798694552236,\n",
    "5798515040940,  5798515335852,  5798515688108,  5798515974828,  5798516286124,  5798694560428,\n",
    "5798515049132,  5798515344044,  5798515696300,  5798515983020,  5798516302508,  5798694568620,\n",
    "5798515057324,  5798515352236,  5798515704492,  5798515991212,  5798516310700,  5798694576812,\n",
    "5798515065516,  5798515360428,  5798515712684,  5798515999404,  5798516318892,  5798694585004,\n",
    "5798515073708,  5798515368620,  5798515720876,  5798516007596,  5798516327084,  5798694617772,\n",
    "5798515081900,  5798515385004,  5798515729068,  5798516015788,  5798516335276,  5798694625964,\n",
    "5798515090092,  5798515393196,  5798515737260,  5798516023980,  5798516343468,  5798694634156,\n",
    "5798515098284,  5798515401388,  5798515745452,  5798516032172,  5798516351660,  5798694642348,\n",
    "5798515106476,  5798515409580,  5798515753644,  5798516048556,  5798516368044,\n",
    "5798515114668,  5798515417772,  5798515761836,  5798516056748,  5798516376236,\n",
    "5798515122860,  5798515434156,  5798515770028,  5798516064940,  5798516384428    \n",
    "]\n",
    "\n",
    "include_dirs = set(include_dirs)\n",
    "\n",
    "include_dirs = [str(x) for x in include_dirs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612ebe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = [\"/home/qb/Repos/experiments/Dataset/\"]\n",
    "\n",
    "include_ids = {1: [{\"fn\": \"C1-1001_performance_20220915103555.csv\"}], \n",
    "               2: [{\"fn\": 'C1-1001_performance_20220915151041.csv'}]}\n",
    "\n",
    "# include_ids = {1: [{\"fn\": \"toy_1.csv\"}], \n",
    "#                2: [{\"fn\": 'toy_2.csv'}]}\n",
    "\n",
    "masks = ['steering_percentage', 'fr_pressure', 'chassis_drive_mode', \n",
    "         'pitch', 'position_y', 'yaw_rate', 'real_axe_pressure', 'roll', \n",
    "         'rr_pressure', 'retarder_torque_feedback', 'control_cmd_brake', \n",
    "         'position_z','speed_FL', 'position_x', 'cmd_drive_mode', 'speed_RL', \n",
    "         'test_id', 'steering_target', 'front_axe_pressure', 'retarder_torque', \n",
    "         'imu_vel_forward', 'throttle', 'limited_retarder_torque', 'speed_RR', \n",
    "         'acc_cmd', 'yaw', 'heading', 'rl_pressure', 'imu_sideshift_acceleration', \n",
    "         'throttle_percentage', 'speed_FR', 'fl_pressure', 'driving_mode', 'timestamp']\n",
    "\n",
    "normalized_features = ['engine_rpm', 'imu_forward_acceleration', \"chassis_acc\", \n",
    "                       \"vehicle_speed\", 'drive_motor_torque_nm', 'speed_cmd']\n",
    "\n",
    "onehot_features = ['gear_location_num']\n",
    "\n",
    "filtermasks=[\"driving_mode\"],\n",
    "filtervalues=[[1, 4]],\n",
    "\n",
    "batch_size = 10\n",
    "seq_length = 100\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcf3431",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = GaussianPreprocessor(data_paths, \n",
    "                                    masks=masks, \n",
    "                                    normalized_features=normalized_features, \n",
    "                                    onehot_features=onehot_features)\n",
    "\n",
    "dataset = SequenceReconstructionDataset(data_paths, \n",
    "                                        batch_size=batch_size,\n",
    "                                        seq_length = seq_length,\n",
    "                                        preprocessor=preprocessor, \n",
    "                                        include_ids=include_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8187088e",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dim = dataset.sample().shape[-1]\n",
    "init_learning_rate = 0.1\n",
    "final_learning_rate = 0.001\n",
    "num_steps = 50\n",
    "beta1 = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbecbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = DependentMaternGP(\n",
    "    nu=1.5,\n",
    "    obs_dim=obs_dim,\n",
    "    length_scale_init=1.5 * torch.ones(obs_dim),\n",
    ").double()\n",
    "\n",
    "    # set up optimizer\n",
    "adam = torch.optim.Adam(\n",
    "    gp.parameters(),\n",
    "    lr=init_learning_rate,\n",
    "    betas=(beta1, 0.999),\n",
    "    amsgrad=True,\n",
    ")\n",
    "    # we decay the learning rate over the course of training\n",
    "gamma = (final_learning_rate / init_learning_rate) ** (1.0 / num_steps)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(adam, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac6d676",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(num_steps):\n",
    "    adam.zero_grad()\n",
    "    losses = []\n",
    "    for batch in dataset.batches():\n",
    "        for x in batch:\n",
    "            loss = -gp.log_prob(x).sum() / seq_length\n",
    "            loss.backward()\n",
    "            losses.append(loss.item())\n",
    "    adam.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    print(\"[step %03d]  loss: %.3f\" % (step, sum(losses) / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb4b4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadCSV(root: str):\n",
    "    csv_files = []\n",
    "\n",
    "# Iterate directory\n",
    "    for path in os.listdir(root):\n",
    "        # check if current path is a file\n",
    "        if os.path.isfile(os.path.join(root, path)) and path.endswith('.csv'):\n",
    "            csv_files.append(path)\n",
    "            \n",
    "    raw_data = None\n",
    "    \n",
    "    for f in csv_files:\n",
    "        if raw_data is not None:\n",
    "            raw_data = pd.concat([raw_data, pd.read_csv(os.path.join(root, path))], axis=0, ignore_index=True)\n",
    "        else:\n",
    "            raw_data = pd.read_csv(os.path.join(root, path))\n",
    "    \n",
    "    raw_data.reset_index()\n",
    "    return raw_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59437fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_multistep = 49\n",
    "T_onestep = 5\n",
    "\n",
    "        # do rolling prediction\n",
    "print(\"doing one-step-ahead forecasting...\")\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "test = dataset.sample().squeeze()\n",
    "\n",
    "print(test.shape)\n",
    "\n",
    "T_onestep = 5\n",
    "\n",
    "\n",
    "onestep_means, onestep_stds = np.zeros((T_onestep, obs_dim)), np.zeros((T_onestep, obs_dim))\n",
    "        \n",
    "for t in range(T_onestep):\n",
    "            # predict one step into the future, conditioning on all previous data.\n",
    "            # note that each call to forecast() conditions on more data than the previous call\n",
    "    dts = torch.tensor([1.0]).double()\n",
    "    pred_dist = gp.forecast(test[0 : T_onestep + t, :], dts)\n",
    "    onestep_means[t, :] = pred_dist.loc.data.numpy()\n",
    "    onestep_stds[t, :] = pred_dist.covariance_matrix.diagonal(dim1=-1, dim2=-2).data.numpy()\n",
    "    \n",
    "    \n",
    "test_y = test[T_onestep:, :]\n",
    "\n",
    "ts = np.linspace(0, test.shape[0] - T_onestep, test.shape[0] - T_onestep);\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "        \n",
    "    f = plt.figure(figsize=(10, 30))\n",
    "    for i in range(5):\n",
    "        ax = f.add_subplot(5, 1, i+1)\n",
    "\n",
    "        # Get upper and lower confidence bounds\n",
    "#         lower, upper = observed_pred.confidence_region()\n",
    "        # Plot training data as black stars\n",
    "        ax.plot(ts, test_y[:, i], 'k*', markersize=2, label='predictions')\n",
    "        # Plot predictive means as blue line\n",
    "        ax.plot(ts, onestep_means[:, i], 'bo', markersize=2, label='observed data')\n",
    "        # Shade between the lower and upper confidence bounds\n",
    "#         ax.plot(overlap_t, overlap_y[:, i], \"rx\", markersize=20, label='overlap between trainset and testset')\n",
    "#         ax.set_title(observ_columns[i])\n",
    "#         ax.fill_between(ts, lowers[:, i], uppers[:, i], alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92aaf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ecd3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821f7bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adlab",
   "language": "python",
   "name": "adlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
