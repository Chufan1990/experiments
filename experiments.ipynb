{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d593d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from urllib.request import urlopen\n",
    "\n",
    "sys.path.append(\"/home/qb/Desktop/GPs\")\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "from data import GaussianPreprocessor, SequenceDataset\n",
    "from models import LSTMFeatureExtractor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1063fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data():\n",
    "    if not os.path.exists(\"eeg.dat\"):\n",
    "        url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/00264/EEG%20Eye%20State.arff\"\n",
    "        with open(\"eeg.dat\", \"wb\") as f:\n",
    "            f.write(urlopen(url).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cc96604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[raw data shape] (14980, 15)\n",
      "[data shape after thinning] torch.Size([749, 14])\n"
     ]
    }
   ],
   "source": [
    "if ('CI' in os.environ):  # this is for running the notebook in our testing framework\n",
    "    train_set = torch.utils.data.TensorDataset(torch.randn(8, 3, 32, 32), torch.rand(8).round().long())\n",
    "    test_set = torch.utils.data.TensorDataset(torch.randn(4, 3, 32, 32), torch.rand(4).round().long())\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=4, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=2, shuffle=False)\n",
    "    num_classes = 2\n",
    "else:\n",
    "    download_data()\n",
    "    T_forecast = 349\n",
    "    data = np.loadtxt(\"eeg.dat\", delimiter=\",\", skiprows=19)\n",
    "    print(\"[raw data shape] {}\".format(data.shape))\n",
    "    data = torch.tensor(data[::20, :-1]).float()\n",
    "    print(\"[data shape after thinning] {}\".format(data.shape))\n",
    "    \n",
    "    T, obs_dim = data.shape\n",
    "    T_train = T - T_forecast\n",
    "\n",
    "    # standardize data\n",
    "    data_mean = data[0:T_train, :].mean(0)\n",
    "    data -= data_mean\n",
    "    data_std = data[0:T_train, :].std(0)\n",
    "    data /= data_std\n",
    "\n",
    "    torch.manual_seed(1080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58923a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "dts = (1 + torch.arange(10)).double()\n",
    "print(dts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59a87d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size=10\n",
    "# train_T = 5\n",
    "# forecast_T = batch_size - train_T\n",
    "\n",
    "# num_batches = data.shape[0] - batch_size\n",
    "\n",
    "# num_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aea4471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMEncoderDecoder(\n",
      "  (lstm_encoder): LSTMFeatureExtractor(\n",
      "    (rnn): LSTM(14, 64, num_layers=2, dropout=0.2, bidirectional=True)\n",
      "  )\n",
      "  (lstm_decoder): LSTMFeatureExtractor(\n",
      "    (rnn): LSTM(128, 64, proj_size=14, num_layers=2, dropout=0.2)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# class LSTMEncoderDecoder(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(LSTMEncoderDecoder, self).__init__()\n",
    "#         self.lstm_encoder = LSTMFeatureExtractor(14, 64, 0, num_layers=2, bidirectional=True, dropout=0.2)\n",
    "#         self.lstm_decoder = LSTMFeatureExtractor(2 * 64, 64, 14, num_layers=2, bidirectional=False, dropout=0.2)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x, (h, c) = self.lstm_encoder(x)\n",
    "#         return self.lstm_decoder(x)\n",
    "    \n",
    "# model = LSTMEncoderDecoder().cuda()\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ec8fdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up optimizer\n",
    "adam = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.1,\n",
    "    )\n",
    "    # we decay the learning rate over the course of training\n",
    "gamma = (0.001 / 0.01) ** (\n",
    "        1.0 / num_epochs\n",
    "    )\n",
    "    \n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(adam, gamma=gamma)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83de41a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0b6b387f894aea84adf0c0e8c1cf9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# epochs_iter = tqdm.notebook.tqdm(range(num_epochs), desc=\"Epoch\")\n",
    "# for i in epochs_iter:\n",
    "#     # Within each iteration, we will go over each minibatch of data\n",
    "    \n",
    "#     adam.zero_grad()\n",
    "    \n",
    "#     start_index = 0\n",
    "    \n",
    "#     losses = []\n",
    "    \n",
    "#     for i in range(num_batches):\n",
    "# #         print(i)\n",
    "#         batch = data[start_index: start_index + train_T, :].cuda()\n",
    "#         target = data[start_index + train_T: start_index + batch_size, :].cuda()\n",
    "#         output, _ = model(batch)\n",
    "#         losses.append(loss_fn(output, target))\n",
    "#         losses[-1].backward()\n",
    "#         start_index += 1\n",
    "    \n",
    "#     total_loss = 0\n",
    "#     for loss in losses:\n",
    "#         total_loss += loss.item()\n",
    "    \n",
    "#     epochs_iter.set_postfix(loss=total_loss)\n",
    "# #     writer.add_scalar('Loss/train', total_loss, i)\n",
    "#     adam.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0282c331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2235, -0.0534, -0.0919, -0.2298, -0.0076, -0.1813, -0.5084, -0.0755,\n",
      "         -0.1404, -0.2393, -0.0631, -0.3437, -0.1596, -0.1141],\n",
      "        [-0.2046, -0.0358, -0.0195, -0.2164,  0.1551, -0.1185, -0.5436, -0.0204,\n",
      "         -0.1387, -0.2142, -0.0543, -0.3537, -0.1565, -0.0977],\n",
      "        [-0.1383,  0.0428,  0.0341, -0.0823,  0.2592,  0.0322, -0.4231,  0.0606,\n",
      "         -0.0928, -0.1328, -0.0286, -0.3320, -0.0992, -0.0806],\n",
      "        [-0.2175, -0.0401, -0.0439, -0.2083,  0.0901, -0.0782, -0.5182,  0.0093,\n",
      "         -0.0894, -0.1775, -0.0429, -0.3433, -0.1478, -0.0808],\n",
      "        [-0.2256, -0.0243, -0.0279, -0.2059,  0.1067, -0.1033, -0.5236,  0.0020,\n",
      "         -0.0871, -0.2036, -0.0162, -0.3664, -0.1454, -0.1068]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "tensor([[ 4.2850e-01,  4.5847e-01,  3.2603e-01,  3.8882e-01, -4.7573e-01,\n",
      "         -6.6099e-01, -8.7429e-02, -1.0739e+00, -3.5041e-01,  1.5334e-01,\n",
      "         -1.6457e+00,  3.3230e-01,  2.2536e-01,  2.9109e-01],\n",
      "        [-2.5319e-01, -1.5004e-01, -7.0540e-01, -2.0315e-03, -2.5928e-01,\n",
      "         -4.5153e-01, -4.3102e-01, -1.3705e+00, -9.7829e-01, -8.7506e-01,\n",
      "         -2.8092e+00, -9.5168e-01, -4.0947e-01, -5.6105e-01],\n",
      "        [ 2.0454e+00,  2.4322e+00,  1.3575e+00,  2.2218e+00, -4.7573e-01,\n",
      "         -1.7525e+00, -9.4668e-01, -2.3349e+00, -2.6668e+00, -1.7750e+00,\n",
      "         -1.7850e+00,  1.0351e+00,  7.1251e-01,  1.7891e+00],\n",
      "        [ 3.9161e+00,  2.4688e+00,  3.2536e+00,  2.8231e+00, -1.9917e+00,\n",
      "         -2.0462e+00, -7.4611e-01, -7.1819e-02,  1.9940e-01,  1.7602e+00,\n",
      "          8.4419e-01,  3.6948e+00,  3.3849e+00,  3.9069e+00],\n",
      "        [ 1.5375e+00, -5.3737e-01,  2.9293e-01,  4.1870e-01, -2.4783e+00,\n",
      "         -1.4162e+00,  5.6154e-02,  1.0409e+00,  1.2209e+00,  2.0816e+00,\n",
      "          3.7907e-01,  1.4941e+00,  3.0011e+00,  2.3315e+00]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# test_x = data[100: 100 + train_T, :]\n",
    "# test_y = data[100 + train_T: 100 + batch_size, :]\n",
    "\n",
    "\n",
    "# pred, _ = model(test_x.cuda())\n",
    "\n",
    "# print(pred)\n",
    "# print(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0761a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3d83d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
